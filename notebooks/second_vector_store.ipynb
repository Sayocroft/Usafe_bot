{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load data from the general PDF\n",
    "def load_pdf_data(pdf_path):\n",
    "    \"\"\"\n",
    "    Function to load text data from a PDF file.\n",
    "    \"\"\"\n",
    "    loader = PyPDFLoader(file_path=pdf_path)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "# Load the general PDF\n",
    "general_documents = load_pdf_data(pdf_path='/Users/sayo/personal_projects/Usafe_bot/data/general_one.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split the loaded document into chunks\n",
    "def split_documents(documents, chunk_size=800, chunk_overlap=80):\n",
    "    \"\"\"\n",
    "    Function to split documents into chunks of given size and overlap.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents=documents)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks in general document: 22\n"
     ]
    }
   ],
   "source": [
    "# Split the general document into chunks\n",
    "general_one_chunks = split_documents(general_documents)\n",
    "print(f\"Number of chunks in general document: {len(general_one_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the embedding vector database for the second vector store\n",
    "def create_embedding_vector_db(chunks, db_name):\n",
    "    \"\"\"\n",
    "    Function to create embeddings and store them in a vector database using FAISS.\n",
    "    \"\"\"\n",
    "    embedding = HuggingFaceEmbeddings(\n",
    "        model_name='sentence-transformers/all-mpnet-base-v2'\n",
    "    )\n",
    "    vectorstore = FAISS.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embedding\n",
    "    )\n",
    "    vectorstore.save_local(f\"./vector_databases/vector_db_{db_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sayo/personal_projects/Usafe_bot/.venv/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# Create the vector store specifically for the general information\n",
    "create_embedding_vector_db(chunks=general_one_chunks, db_name='usafe_general')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve data from the new vector store\n",
    "def retrieve_from_vector_db(vector_db_path):\n",
    "    \"\"\"\n",
    "    Function to get a retriever object from the new vector database.\n",
    "    \"\"\"\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name='sentence-transformers/all-mpnet-base-v2'\n",
    "    )\n",
    "    react_vectorstore = FAISS.load_local(\n",
    "        folder_path=vector_db_path,\n",
    "        embeddings=embeddings,\n",
    "        allow_dangerous_deserialization=True\n",
    "    )\n",
    "    retriever = react_vectorstore.as_retriever()\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve from the newly created vector store for general information\n",
    "general_retriever = retrieve_from_vector_db(vector_db_path='./vector_databases/vector_db_usafe_general')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.vectorstores.base.VectorStoreRetriever"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(general_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_chains(retriever):\n",
    "    \"\"\"\n",
    "    this function connects stuff_documents_chain with retrieval_chain\n",
    "    \"\"\"\n",
    "    stuff_documents_chain = create_stuff_documents_chain(\n",
    "        llm=llm,\n",
    "        prompt=hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "    )\n",
    "    retrieval_chain = create_retrieval_chain(\n",
    "        retriever=retriever,\n",
    "        combine_docs_chain=stuff_documents_chain\n",
    "    )\n",
    "    return retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama3-8b-8192\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain = connect_chains(retriever=general_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Document Content: • Alternative Support: If you don’t know someone who can assist, you may contact organizations that provide translation support for hate crime victims. One example is ReachOut Berlin, which offers assistance for individuals facing hate crime incidents. • Contact: • ReachOut Berlin • Email: info@reachoutberlin.de • Address: Oranienburger Str. 27, 10117 Berlin 4. Visit Your Local Police Station: • Bring all collected documentation with you. Explain the details of the incident, and let the officer know you believe it to be a hate crime. • The police will create an official report based on your statement and evidence. 5. Report Online (Optional): • If you’re unable to visit the police station, you may be able to file a report online through local authorities’ websites or specific online\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Function to clean the extracted text by removing extra line breaks and spaces.\n",
    "    \"\"\"\n",
    "    # Replace line breaks with spaces and remove extra spaces\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = \" \".join(text.split())  # Removes multiple spaces\n",
    "    return text\n",
    "\n",
    "# Test retrieval with cleaned output\n",
    "def print_output(inquiry, retriever=general_retriever):\n",
    "    \"\"\"\n",
    "    Function to test the retrieval chain with a query and clean up the output.\n",
    "    \"\"\"\n",
    "    docs = retriever.get_relevant_documents(inquiry)\n",
    "    if docs:\n",
    "        # Clean the retrieved document content\n",
    "        cleaned_text = clean_text(docs[0].page_content)\n",
    "        print(\"Cleaned Document Content:\", cleaned_text)\n",
    "    else:\n",
    "        print(\"No relevant information found.\")\n",
    "\n",
    "# Example query to test the retrieval\n",
    "print_output(\"How do I report a hate crime?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Document Content:\n",
      "\n",
      "\n",
      "• Alternative Support: If you don’t know someone who can assist, you may contact organizations that provide translation support for hate crime victims. One example is ReachOut Berlin, which offers assistance for individuals facing hate crime incidents. \n",
      "• Contact: \n",
      "• ReachOut Berlin \n",
      "• Email: info@reachoutberlin.de \n",
      "• Address: Oranienburger Str. 27, 10117 Berlin \n",
      "4. Visit Your Local Police Station: \n",
      "• Bring all collected documentation with you. Explain the details of the incident, and let the officer know you believe it to be a hate crime. \n",
      "• The police will create an official report based on your statement and evidence. \n",
      "5. Report Online (Optional): \n",
      "• If you’re unable to visit the police station, you may be able to file a report online through local authorities’ websites or specific online\n"
     ]
    }
   ],
   "source": [
    "def clean_and_format_text(text):\n",
    "    \"\"\"\n",
    "    Clean the extracted text and format it for better readability.\n",
    "    \"\"\"\n",
    "    # Replace newlines with spaces and remove multiple spaces\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = \" \".join(text.split())\n",
    "    \n",
    "    # Add line breaks after bullets and section numbers for better readability\n",
    "    text = text.replace(\"•\", \"\\n•\")\n",
    "    text = text.replace(\"1.\", \"\\n1.\")\n",
    "    text = text.replace(\"2.\", \"\\n2.\")\n",
    "    text = text.replace(\"3.\", \"\\n3.\")\n",
    "    text = text.replace(\"4.\", \"\\n4.\")\n",
    "    text = text.replace(\"5.\", \"\\n5.\")\n",
    "    \n",
    "    return text\n",
    "\n",
    "def print_output(inquiry, retriever=general_retriever):\n",
    "    \"\"\"\n",
    "    Function to test the retrieval chain with a query and display organized output.\n",
    "    \"\"\"\n",
    "    docs = retriever.get_relevant_documents(inquiry)\n",
    "    if docs:\n",
    "        # Clean and format the retrieved document content\n",
    "        cleaned_text = clean_and_format_text(docs[0].page_content)\n",
    "        print(\"Cleaned Document Content:\\n\")\n",
    "        print(cleaned_text)\n",
    "    else:\n",
    "        print(\"No relevant information found.\")\n",
    "\n",
    "# Example query to test the retrieval\n",
    "print_output(\"How do I report a hate crime?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
